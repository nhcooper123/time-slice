Right I've tried to take a step back to really think about what this paper is trying to show. I think some of the confusion is we are trying to show too many things at once. See if you can follow this logic.

We want to demonstrate our new time slicing method(s). We want to show how the method(s) work, and prove that they are superior to stratigraphic and equal sized time bins.

Why are they superior?
1) stratigraphic time bins are not of equal size, biasing higher disparity towards bigger stratigraphic ages. This can be dealt with by rarefaction, but can lead to massive confidence intervals when there are bins with very few species. Also there's some circularity if is statigraphy defined by the species within the rocks [this is right yes?]

[TG: yes totally! One note on the stratigraphy thing, it will need to be written really carefully, I tried to talk about it to Davey, The Bapst and GraEme last Evolution conference and - with my bad communication skills - they weren't convinced by the circularity]

2) time bins ignore ancestors? [I've forgotten the motivation here tbh]

[TG: From one of the reviewers: "Currently most palaeobiologists are stuck using binned data (e.g., Figure 4) which tends to lead to time series of insufficient length (here N = 3) to explore, for example, correlations with climate proxies (or some other hypothesised driver of morphological diversity). Time-slicing could revolutionise the field by opening up data sets to such analyses and an example alongside the binning in Figure 4 should be considered.", Basically, adding the ancestor allows for more data. Many caveats there but still good. I have to find back a reference from Butler or Brusatte that advocate for such method (but don't implement it)]

What are our questions/what do we want to prove?
1 - We want to "prove" our method is "better"
2 - We want to show differences in different kinds of time slices.

OK I think you've gotten stuck into 2, which is comparing all those curves on your plots. But actually we want to focus first on 1. 

1 also has multiple components - we want to compare for stratigraphic times, what happens when we time slice instead. But we *also* want to compare differences between stratigraphy, equal duration bins, equal numbers of bins and time slices for each of those options. Basically it's a total cluster fuck. There's comparison within plots and between plots.

[TG: I agree on technical term "cluster fuck"!]

So taking a step back again, what exactly do we want to show? What do people use these disparity analyses to show that we think might be dodgy? 

a) how disparity changes through time. 
e.g. primate disparity is really low initially, then peaks at the PETM, then decreases again.

I agree looking at the overall shape of the curve is the ideal thing, but actually it's hard to do any stats on. When we did the mammal paper we used t tests and we were very unhappy with them. And most people don't even bother with stats. Differences in the areas under the curves works to some extent, as it shows which methods capture more disparity, but actually doesn't tell you much about the change through time, i.e. the shape of that curve. My worry with area is you could get the same area with a curve that goes up, then down, as for the mirror image that goes down then up. Also it's hard to do. My stats collaborator just shook his head and said it's basically not possible. 

The curves aren't curves anyway. They're joined up points. We only actually estimate disparity at certain time poitns, not continuously through the tree. So really we should be looking to compare points. I wonder if there is therefore a simple solution - just do paired Wilcoxon rank sum tests on the paired disparities between two methods?

[TG: yes, this could work. I think it's not statistically exciting (boring, boring wilcox.test) but you're right on the point that most people don't even bother with stats!]

b) Max (and min?) disparity points
Another thing I think is key, thinking about what people use these methods for, is to note the time points we see highest and lowest disparity. We can probably do something as simple as a chi squared to get a p value for whether a certain method consistently over or under estimates these. Biologically speaking this is important - as peak disaprity is something people report. If it shifts that is a big deal.

[TG: this is a good idea! I think we could even simplify it without stats: "does the lowest and highest disparity value occur at the same time?" in other words, within all the methods we're testing, do we see the min/max disparity always at the same time or does on method show min/max disparity later/earlier than the others. Linking back to your thought, that is the important stuff here: e.g. in the K-Pg mammal stuff, we want to see if there is a peak (low or high) at a specific geological time. Do different methods show this peak at different time (if yes we can argue that this peak is maybe shite).]

c) Finally people are interested in pre and post extinction events. I don't think many of our datasets allow us to test this, but we can do it with Beck at least, and just do a t.test on before and after KPg, like you did in your thesis.

[TG: that could be a good anectodale point yes. Maybe something to put in the discussion: "people are interested in post/pre extinction and even for that different methods show different results (or not - e.g. in the Beck data set it shows blabalbalbal)"]

2. Differences in different kinds of time slices
We can probably test this in the same way. We probably want to choose just one slice method to report for part 1. This can maybe go in an appendix if there isn't space.

[TG: concerning the different time slice models as I told you, I've worked on completely new methods to estimate the value along the branch.
Basically it all boils down to that: the disparity inference between tips/nodes. No body will argue on the observed disparity when tips/nodes are available.
Until now, we approached the problem by choosing the data on the branch to be either the ancestor or the descendant: regardless of the model, we don't create *chimeras* (e.g. 70% ancestor + 30% descendant) and always pick either.
That was one of the (right) comments of one of the reviewers that a cool model would be to actually create these chimeras.
So I've done so models renaming to be more accurate: the four models we're using until now are renamed to:
 - "fixed" models: ACCTRAN, DELTRAN, punctuated (that is now called "random") and gradual (that is now called "proximity"). These models choose *once* between the ancestor or the descendant so that every time you measure disparity it's always on the same set of tips/nodes.
 - the new models are called "probability" models: punctuated and gradual (the real ones). These models are different in that they choose *both* ancestor/descendant but with a certain probability each time (either an equal probability for punctuated or one proportional to branch length for the gradual). This means that each time you measure disparity, it can choose different tips/nodes depending on their probability. This is problematic when measuring observed disparity (i.e. no bootstraps) because it might change each time (depending on the probabilities) but will average out when bootstrapped: i.e. for one slice with the probability of sampling the descendant of 70% and the ancestor for 30%, actually 70% of the descendant data (and 30% of ancestor) will be present in the bootstrapped disparity value. This contrast with the former punctuated/gradual models where the slice had a probability of choosing either the ancestor or the descendant first and then always bootstrap the same ancestor/descendant data. (https://rawgit.com/TGuillerme/dispRity/master/inst/gitbook/_book/details-of-specific-functions.html#time-slicing)

Because I'm in a rush today, I will try to only fix what you've asked (the pipeline for getting the disparity data) but next week, I'll get some quiet time to rewrite the code on a new branch using the latest dispRity version AND the two new models.

What do you think?]








